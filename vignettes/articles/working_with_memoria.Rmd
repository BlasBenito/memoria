---
title: "Working with memoria"
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 9,
  fig.height = 4,
  out.width = "100%"
)
```

This article walks through the complete memoria workflow for quantifying ecological memory in time series data consisting of at least one response and one predictor.

## Ecological Memory

The package `memoria` helps quantify how past and concurrent events shape the state of a system. This concept is summarized in the figure below:

![Ecological Memory](ecological_memory.png)
The blue and green curves represent the values of a biotic response `y` and a driver `x` over time. The vertical line `T` represents the *current* time, while `T - lag` represents a a previous time. The arrows represent *influence*, as follows:

 + **Endogenous memory**: is the effect of past `y` on present `y`.
 + **Exogenous memory**: is the effect of past `x` on present `y`.
 + **Concurrent effect**: is the effect of present `x` on present `y`.
 
To assess these effects, `memoria` generates the lagged data and trains the model

$$y_{T} = y_{T-1} + \ldots + y_{T-n} + x_{T} + x_{T-1} + \ldots + x_{T-n} + r_{T}$$

Where:

- $y$ and $x$ are the response and the driver.
- $T$ is the "current" time, and $t-1$ to  $t-n$, are the time lags.
- $y_{T-1} + \ldots + y_{T-n}$ represents the **endogenous memory** across lags.
- $x_{T-1} + \ldots + x_{T-n}$ represents the **exogenous memory** across lags.
- $x_{T}$ represents the **concurrent effect**.
- $r_{T}$ is a random term used to assess statistical significance.

The ecological memory pattern is quantified using **permutation importance** from Random Forest models. For each predictor, the algorithm shuffles its values and measures the decrease in predictive accuracyâ€”larger decreases indicate stronger influence on the response.

![Ecological Memory](memory_plot.png)

The following sections demonstrate how to implement this workflow in R using the `memoria` package.

## Setup

```{r install-deps, include = FALSE}
for (pkg in c("tidyr", "knitr")) {
  if (!requireNamespace(pkg, quietly = TRUE)) {
    install.packages(pkg)
  }
}
```

```{r load-packages, message = FALSE, warning = FALSE}
library(memoria)
library(ggplot2)
library(tidyr)
library(knitr)
```



## Workflow

### Merge datasets

The `mergePalaeoData()` function aligns and interpolates datasets sampled at different resolutions. Consider two built-in datasets:

- **pollen**: 639 samples with four pollen types (Pinus, Quercus, Poaceae, Artemisia)
- **climate**: 800 samples with climate variables (temperature, rainfall, etc.)

```{r plot-pollen}
data(pollen)

ggplot(
  data = gather(pollen, pollen.type, pollen.abundance, 2:5),
  aes(x = age, y = pollen.abundance, group = pollen.type)
) +
  geom_line() +
  facet_wrap(vars(pollen.type), ncol = 1, scales = "free_y") +
  xlab("Age (ky BP)") +
  ylab("Pollen counts") +
  ggtitle("Pollen dataset")
```

```{r plot-climate}
data(climate)

ggplot(
  data = gather(climate, variable, value, 2:5),
  aes(x = age, y = value, group = variable)
) +
  geom_line() +
  facet_wrap(vars(variable), ncol = 1, scales = "free_y") +
  xlab("Age (ky BP)") +
  ylab("") +
  ggtitle("Palaeoclimatic data")
```

Merge and interpolate into a regular time grid:

```{r merge-data, fig.height = 8}
pollen_climate <- mergePalaeoData(
  datasets.list = list(
    pollen = pollen,
    climate = climate
  ),
  time.column = "age",
  interpolation.interval = 0.2
)

str(pollen_climate)

ggplot(
  data = gather(pollen_climate, variable, value, 2:10),
  aes(x = age, y = value, group = variable)
) +
  geom_line() +
  facet_wrap(vars(variable), ncol = 1, scales = "free_y") +
  xlab("Age (ky BP)") +
  ylab("") +
  ggtitle("Pollen and palaeoclimate (merged)")
```

### Create lagged data

The `prepareLaggedData()` function organizes data into the lag structure required by the model. Each sample of the response is aligned with antecedent values of itself and the drivers.

```{r prepare-lagged}
pollen_climate_lagged <- prepareLaggedData(
  input.data = pollen_climate,
  response = "pollen.pinus",
  drivers = c("climate.temperatureAverage", "climate.rainfallAverage"),
  time = "age",
  oldest.sample = "last",
  lags = seq(0.2, 1, by = 0.2)
)

str(pollen_climate_lagged)

# Check attributes (used by computeMemory for auto-detection)
attr(pollen_climate_lagged, "response")
attr(pollen_climate_lagged, "drivers")
```

**Note on `oldest.sample`:** Set to `"last"` when the oldest sample is at the bottom of the dataframe (typical palaeoecological convention). Set to `"first"` if the oldest sample is at the top.

The output columns follow a naming convention:

- `pollen.pinus__0`: Current response value
- `pollen.pinus__0.2`, `pollen.pinus__0.4`, etc.: Endogenous terms
- `climate.temperatureAverage__0`, `climate.temperatureAverage__0.2`, etc.: Concurrent effect and exogenous terms

### Compute memory

The `computeMemory()` function fits Random Forest models on the lagged data. It measures variable importance using permutation, which is robust to multicollinearity and temporal autocorrelation.

Two benchmark modes are available for significance testing:

- `"white.noise"`: Random values without temporal structure (faster)
- `"autocorrelated"`: Random walk with temporal structure (more conservative, recommended)

The model runs multiple times (`repetitions`), regenerating the random term each time, then computes percentiles of importance scores.

```{r compute-memory-whitenoise, fig.width = 5, fig.height = 2}
# Simplified call - response and drivers auto-detected from lagged data attributes
memory_output <- computeMemory(
  lagged.data = pollen_climate_lagged,
  random.mode = "white.noise",
  repetitions = 100
)

# Output structure (5 slots)
names(memory_output)

# Memory dataframe (lowercase column names: variable, lag)
head(memory_output$memory)

# Pseudo R-squared
head(memory_output$R2)

# Plot
plotMemory(memory_output)
```

Now with autocorrelated random mode (more stringent):

```{r compute-memory-autocor, fig.width = 5, fig.height = 2}
memory_output_autocor <- computeMemory(
  lagged.data = pollen_climate_lagged,
  random.mode = "autocorrelated",
  repetitions = 100
)

plotMemory(memory_output_autocor)
```

Notice the yellow band (random benchmark) is higher with autocorrelated mode, providing a more conservative significance threshold.

### Extract memory features

The `extractMemoryFeatures()` function summarizes memory patterns into quantitative features:

```{r extract-features}
memory_features <- extractMemoryFeatures(
  memory.pattern = memory_output_autocor,
  exogenous.component = c(
    "climate.temperatureAverage",
    "climate.rainfallAverage"
  ),
  endogenous.component = "pollen.pinus",
  sampling.subset = NULL,
  scale.strength = TRUE
)

kable(memory_features)
```

## Understanding the Output

### Significance testing

Random Forest provides importance scores but no built-in significance test. The solution is to add a random benchmark variable: if a predictor's importance equals or falls below the random variable's importance, we cannot distinguish its contribution from noise.

In plots, predictors with importance above the yellow band (random median) are considered significant. The width of the band reflects uncertainty across model repetitions.

The autocorrelated benchmark is recommended for palaeoecological analysis because it accounts for the temporal structure inherent in these datasets.

### Memory features

The extracted features quantify three aspects of ecological memory:

- **Strength**: Maximum importance difference between a component and the random median. Higher values indicate stronger memory effects. Computed for endogenous, exogenous, and concurrent components.

- **Length**: Proportion of lags where importance exceeds the random median. Values near 1 indicate persistent memory across all lags. Computed for endogenous and exogenous components only.

- **Dominance**: Among significant lags, the proportion where one component (endogenous or exogenous) exceeds the other. Values near 1 indicate one component dominates; values near 0.5 indicate balance.

## Batch Experiments

The memoria package includes functions for batch analysis of simulated pollen curves from the [virtualPollen](https://github.com/BlasBenito/virtualPollen) package:

- `runExperiment()`: Apply the full workflow across multiple simulations
- `experimentToTable()`: Reshape experiment results into long format
- `plotExperiment()`: Visualize memory patterns across parameter combinations

These functions enable systematic exploration of how ecological memory varies with species traits and sampling resolution. The virtualPollen package must be installed separately from GitHub (`devtools::install_github("BlasBenito/virtualPollen")`).

## Summary

The complete workflow:

1. `mergePalaeoData()` - Align datasets with different resolutions
2. `prepareLaggedData()` - Create lag structure
3. `computeMemory()` - Fit Random Forest and compute importance
4. `plotMemory()` - Visualize the memory pattern
5. `extractMemoryFeatures()` - Quantify memory characteristics
