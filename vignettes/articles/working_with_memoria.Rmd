---
title: "Working with memoria"
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 9,
  fig.height = 4,
  out.width = "100%"
)
```

This article walks through the complete memoria workflow for quantifying ecological memory in palaeoecological datasets.

## Setup

```{r load-packages, message = FALSE, warning = FALSE}
library(memoria)
library(ggplot2)
library(tidyr)
library(knitr)
```

## The Model

Ecological memory analysis fits a model of the form:

$$p_{t} = p_{t-1} + \ldots + p_{t-n} + d_{t} + d_{t-1} + \ldots + d_{t-n}$$

Where:

- $p$ is the biotic response (e.g., pollen abundance)
- $d$ is the environmental driver
- $t$ is time; $t-1$, $t-2$, etc. are time lags
- $p_{t-1} + \ldots + p_{t-n}$ represents **endogenous memory** (effect of past response on current state)
- $d_{t-1} + \ldots + d_{t-n}$ represents **exogenous memory** (effect of past drivers on current state)
- $d_{t}$ represents the **concurrent effect** (synchronic driver influence)

Random Forest returns importance scores for each term, which memoria plots and summarizes.

## Workflow

### Merge datasets

The `mergePalaeoData()` function aligns and interpolates datasets sampled at different resolutions. Consider two built-in datasets:

- **pollen**: 639 samples with four pollen types (Pinus, Quercus, Poaceae, Artemisia)
- **climate**: 800 samples with climate variables (temperature, rainfall, etc.)

```{r plot-pollen}
data(pollen)

ggplot(
  data = gather(pollen, pollen.type, pollen.abundance, 2:5),
  aes(x = age, y = pollen.abundance, group = pollen.type)
) +
  geom_line() +
  facet_wrap(vars(pollen.type), ncol = 1, scales = "free_y") +
  xlab("Age (ky BP)") +
  ylab("Pollen counts") +
  ggtitle("Pollen dataset")
```

```{r plot-climate}
data(climate)

ggplot(
  data = gather(climate, variable, value, 2:5),
  aes(x = age, y = value, group = variable)
) +
  geom_line() +
  facet_wrap(vars(variable), ncol = 1, scales = "free_y") +
  xlab("Age (ky BP)") +
  ylab("") +
  ggtitle("Palaeoclimatic data")
```

Merge and interpolate into a regular time grid:

```{r merge-data, fig.height = 8}
pollen_climate <- mergePalaeoData(
  datasets.list = list(
    pollen = pollen,
    climate = climate
  ),
  time.column = "age",
  interpolation.interval = 0.2
)

str(pollen_climate)

ggplot(
  data = gather(pollen_climate, variable, value, 2:10),
  aes(x = age, y = value, group = variable)
) +
  geom_line() +
  facet_wrap(vars(variable), ncol = 1, scales = "free_y") +
  xlab("Age (ky BP)") +
  ylab("") +
  ggtitle("Pollen and palaeoclimate (merged)")
```

### Create lagged data

The `prepareLaggedData()` function organizes data into the lag structure required by the model. Each sample of the response is aligned with antecedent values of itself and the drivers.

```{r prepare-lagged}
pollen_climate_lagged <- prepareLaggedData(
  input.data = pollen_climate,
  response = "pollen.pinus",
  drivers = c("climate.temperatureAverage", "climate.rainfallAverage"),
  time = "age",
  oldest.sample = "last",
  lags = seq(0.2, 1, by = 0.2)
)

str(pollen_climate_lagged)

# Check attributes (used by computeMemory for auto-detection)
attr(pollen_climate_lagged, "response")
attr(pollen_climate_lagged, "drivers")
```

**Note on `oldest.sample`:** Set to `"last"` when the oldest sample is at the bottom of the dataframe (typical palaeoecological convention). Set to `"first"` if the oldest sample is at the top.

The output columns follow a naming convention:

- `pollen.pinus__0`: Current response value
- `pollen.pinus__0.2`, `pollen.pinus__0.4`, etc.: Endogenous terms
- `climate.temperatureAverage__0`, `climate.temperatureAverage__0.2`, etc.: Concurrent effect and exogenous terms

### Compute memory

The `computeMemory()` function fits Random Forest models on the lagged data. It measures variable importance using permutation, which is robust to multicollinearity and temporal autocorrelation.

Two benchmark modes are available for significance testing:

- `"white.noise"`: Random values without temporal structure (faster)
- `"autocorrelated"`: Random walk with temporal structure (more conservative, recommended)

The model runs multiple times (`repetitions`), regenerating the random term each time, then computes percentiles of importance scores.

```{r compute-memory-whitenoise, fig.width = 5, fig.height = 2}
# Simplified call - response and drivers auto-detected from lagged data attributes
memory_output <- computeMemory(
  lagged.data = pollen_climate_lagged,
  random.mode = "white.noise",
  repetitions = 100
)

# Output structure (5 slots)
names(memory_output)

# Memory dataframe (lowercase column names: variable, lag)
head(memory_output$memory)

# Pseudo R-squared
head(memory_output$R2)

# Plot
plotMemory(memory_output)
```

Now with autocorrelated random mode (more stringent):

```{r compute-memory-autocor, fig.width = 5, fig.height = 2}
memory_output_autocor <- computeMemory(
  lagged.data = pollen_climate_lagged,
  random.mode = "autocorrelated",
  repetitions = 100
)

plotMemory(memory_output_autocor)
```

Notice the yellow band (random benchmark) is higher with autocorrelated mode, providing a more conservative significance threshold.

### Extract memory features

The `extractMemoryFeatures()` function summarizes memory patterns into quantitative features:

```{r extract-features}
memory_features <- extractMemoryFeatures(
  memory.pattern = memory_output_autocor,
  exogenous.component = c(
    "climate.temperatureAverage",
    "climate.rainfallAverage"
  ),
  endogenous.component = "pollen.pinus",
  sampling.subset = NULL,
  scale.strength = TRUE
)

kable(memory_features)
```

## Understanding the Output

### Significance testing

Random Forest provides importance scores but no built-in significance test. The solution is to add a random benchmark variable: if a predictor's importance equals or falls below the random variable's importance, we cannot distinguish its contribution from noise.

In plots, predictors with importance above the yellow band (random median) are considered significant. The width of the band reflects uncertainty across model repetitions.

The autocorrelated benchmark is recommended for palaeoecological analysis because it accounts for the temporal structure inherent in these datasets.

### Memory features

The extracted features quantify three aspects of ecological memory:

- **Strength**: Maximum importance difference between a component and the random median. Higher values indicate stronger memory effects. Computed for endogenous, exogenous, and concurrent components.

- **Length**: Proportion of lags where importance exceeds the random median. Values near 1 indicate persistent memory across all lags. Computed for endogenous and exogenous components only.

- **Dominance**: Among significant lags, the proportion where one component (endogenous or exogenous) exceeds the other. Values near 1 indicate one component dominates; values near 0.5 indicate balance.

## Batch Experiments

The memoria package includes functions for batch analysis of simulated pollen curves from the [virtualPollen](https://github.com/BlasBenito/virtualPollen) package:

- `runExperiment()`: Apply the full workflow across multiple simulations
- `experimentToTable()`: Reshape experiment results into long format
- `plotExperiment()`: Visualize memory patterns across parameter combinations

These functions enable systematic exploration of how ecological memory varies with species traits and sampling resolution. The virtualPollen package must be installed separately from GitHub (`devtools::install_github("BlasBenito/virtualPollen")`).

## Summary

The complete workflow:

1. `mergePalaeoData()` - Align datasets with different resolutions
2. `prepareLaggedData()` - Create lag structure
3. `computeMemory()` - Fit Random Forest and compute importance
4. `plotMemory()` - Visualize the memory pattern
5. `extractMemoryFeatures()` - Quantify memory characteristics
