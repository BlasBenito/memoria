---
title: "Quick Start Guide"
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 9,
  fig.height = 4,
  out.width = "100%"
)
```

This guide walks through the complete memoria workflow: preparing data, computing ecological memory patterns, and extracting memory features.

## Setup

```{r load-packages, message = FALSE, warning = FALSE}
library(memoria)
library(ggplot2)
library(tidyr)
library(knitr)
```

## The Model

Ecological memory analysis fits a model of the form:

$$p_{t} = p_{t-1} + \ldots + p_{t-n} + d_{t} + d_{t-1} + \ldots + d_{t-n}$$

Where:

- $p$ is pollen (or any biotic response)
- $d$ is the environmental driver
- $t$ is time; $t-1$, $t-2$, etc. are time lags
- $p_{t-1} + \ldots + p_{t-n}$ represents **endogenous memory**
- $d_{t-1} + \ldots + d_{t-n}$ represents **exogenous memory**
- $d_{t}$ represents the **concurrent effect**

Random Forest returns importance scores for each term, which memoria plots and summarizes.

## Step 1: Prepare Data

### Merge datasets with different temporal resolutions

The `mergePalaeoData()` function aligns and interpolates datasets sampled at different resolutions. Consider two built-in datasets:

- **pollen**: 639 samples with four pollen types (Pinus, Quercus, Poaceae, Artemisia)
- **climate**: 800 samples with climate variables (temperature, rainfall, etc.)

```{r plot-pollen}
data(pollen)

ggplot(
  data = gather(pollen, pollen.type, pollen.abundance, 2:5),
  aes(x = age, y = pollen.abundance, group = pollen.type)
) +
  geom_line() +
  facet_wrap(vars(pollen.type), ncol = 1, scales = "free_y") +
  xlab("Age (ky BP)") +
  ylab("Pollen counts") +
  ggtitle("Pollen dataset")
```

```{r plot-climate}
data(climate)

ggplot(
  data = gather(climate, variable, value, 2:5),
  aes(x = age, y = value, group = variable)
) +
  geom_line() +
  facet_wrap(vars(variable), ncol = 1, scales = "free_y") +
  xlab("Age (ky BP)") +
  ylab("") +
  ggtitle("Palaeoclimatic data")
```

Merge and interpolate into a regular time grid:

```{r merge-data, fig.height = 8}
pollen_climate <- mergePalaeoData(
  datasets.list = list(
    pollen = pollen,
    climate = climate
  ),
  time.column = "age",
  interpolation.interval = 0.2
)

str(pollen_climate)

ggplot(
  data = gather(pollen_climate, variable, value, 2:10),
  aes(x = age, y = value, group = variable)
) +
  geom_line() +
  facet_wrap(vars(variable), ncol = 1, scales = "free_y") +
  xlab("Age (ky BP)") +
  ylab("") +
  ggtitle("Pollen and palaeoclimate (merged)")
```

## Step 2: Create Lagged Data

The `prepareLaggedData()` function organizes data into the lag structure required by the model. Each sample of the response is aligned with antecedent values of itself and the drivers.

```{r prepare-lagged}
pollen_climate_lagged <- prepareLaggedData(
  input.data = pollen_climate,
  response = "pollen.pinus",
  drivers = c("climate.temperatureAverage", "climate.rainfallAverage"),
  time = "age",
  oldest.sample = "last",
  lags = seq(0.2, 1, by = 0.2),
  time.zoom = NULL,
  scale = FALSE
)

str(pollen_climate_lagged)
```

**Note on `oldest.sample`:** Set to `"last"` when the oldest sample is at the bottom of the dataframe (typical palaeoecological convention). Set to `"first"` if the oldest sample is at the top.

The output columns follow a naming convention:

- `Response_0`: Current response value
- `Response_Lag_0.2`, `Response_Lag_0.4`, etc.: Endogenous terms
- `climate.temperatureAverage_Lag_0`, `climate.temperatureAverage_Lag_0.2`, etc.: Concurrent effect and exogenous terms

## Step 3: Compute Memory

The `computeMemory()` function fits Random Forest models on the lagged data. It measures variable importance using permutation, which is robust to multicollinearity and temporal autocorrelation.

To assess significance, the function adds a random benchmark term. Two modes are available:

- `"white.noise"`: Random values without temporal structure
- `"autocorrelated"`: Random walk with temporal structure (more conservative, recommended)

The model runs multiple times (`repetitions`), regenerating the random term each time, then computes percentiles of importance scores.

```{r compute-memory-whitenoise, fig.width = 5, fig.height = 2}
memory_output <- computeMemory(
  lagged.data = pollen_climate_lagged,
  drivers = c("climate.temperatureAverage", "climate.rainfallAverage"),
  response = "Response",
  add.random = TRUE,
  random.mode = "white.noise",
  repetitions = 100
)

# Output structure
names(memory_output)

# Memory dataframe
head(memory_output$memory)

# Pseudo R-squared
head(memory_output$R2)

# Multicollinearity check (VIF > 5 indicates issues)
head(memory_output$multicollinearity)

# Plot
plotMemory(memory_output)
```

Now with autocorrelated random mode (more stringent):

```{r compute-memory-autocor, fig.width = 5, fig.height = 2}
memory_output_autocor <- computeMemory(
  lagged.data = pollen_climate_lagged,
  drivers = c("climate.temperatureAverage", "climate.rainfallAverage"),
  response = "Response",
  add.random = TRUE,
  random.mode = "autocorrelated",
  repetitions = 100
)

plotMemory(memory_output_autocor)
```

Notice the yellow band (random benchmark) is higher with autocorrelated mode, providing a more conservative significance threshold.

## Step 4: Extract Memory Features

The `extractMemoryFeatures()` function summarizes memory patterns into quantitative features:

- **Memory strength**: Maximum difference between component importance and the random median (for endogenous, exogenous, and concurrent)
- **Memory length**: Proportion of lags where importance exceeds the random median (endogenous and exogenous only)
- **Dominance**: Proportion of significant lags where one component exceeds the other (endogenous and exogenous only)

```{r extract-features}
memory_features <- extractMemoryFeatures(
  memory.pattern = memory_output_autocor,
  exogenous.component = c(
    "climate.temperatureAverage",
    "climate.rainfallAverage"
  ),
  endogenous.component = "Response",
  sampling.subset = NULL,
  scale.strength = TRUE
)

kable(memory_features)
```

## Summary

The complete workflow:
1. `mergePalaeoData()` - Align datasets with different resolutions
2. `prepareLaggedData()` - Create lag structure
3. `computeMemory()` - Fit Random Forest and compute importance
4. `plotMemory()` - Visualize the memory pattern
5. `extractMemoryFeatures()` - Quantify memory characteristics

For theoretical background and advanced usage with simulated data, see the [Using memoria](using_memoria.html) article.
